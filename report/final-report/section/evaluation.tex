\section{Experimental Setup} \label{sec:eval}

We address two problems in our experiments :\\

\begin{itemize}
	\item Mapping a SQL workload to a benchmark class.
	\item Estimating the performance of the DBMS using the features and the class
	model without actually executing the workload.\\
\end{itemize}

\begin{table*}
\centering
  \begin{adjustbox}{max width=\textwidth}
  \begin{tabular}{l|lll}
	\toprule
   		Feature name &  Feature meaning & Default value & Mutated value set \\
    \midrule
    	shared\_buffers & Number of shared memory buffers used by the database
    	server & 128 MB &  [ 4 MB, 32 MB, 128 MB, 512 MB ] \\
    	bgwrite\_delay &  Delay between rounds for the
    	background writer & 100 ms &  [ 100 ms, 1000 ms, 10000 ms ] \\
    	wal\_level & Type of logging & minimal &  [ minimal, archive, hot\_standby,
    	logical ] \\
    	fsync & Forced synchronization to disk & on &  [ on, off ] \\
    	synchronous\_commit & Whether transaction commit will wait for WAL records to be flushed & on &  [ on, off, local,
    	remote\_write ] \\
    	wal\_buffers & Number of disk-page buffers in shared memory for WAL &
    	-1 &  [ -1, 4 MB, 32 MB, 128 MB ] \\
    	wal\_writer\_delay & Specifies the delay between rounds for the
    	WAL writer & 200 ms &  [ 100 ms, 200 ms, 1000 ms, 10000 ms] \\
    	commit\_delay & Time delay between writing a commit record to the WAL
    	buffer & 0 $\mu$s &  [ 0 $\mu$s, 1000 $\mu$s, 1000 $\mu$s, 10000 $\mu$s] \\
    	track\_activities & Collect query/index statistics  & on &  [ on, off ]	\\
    	log\_planner\_stats & Log planner statistics  & off &  [ on, off ] \\
    	debug\_print\_plan & Print plans in log  & off &  [ on, off ] \\
    	autovacuum & Enable autovacuum subprocess  & on &  [ on, off ] \\
   \bottomrule
   \end{tabular}
   \end{adjustbox}
\caption{Configuration attributes that we mutate in each DBMS run.}
\label{tab:mutate}
\end{table*}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figure/clustering.pdf}
    \caption{Clusters found by the clustering algorithms.}
    \label{fig:clusters}
\end{figure*}

\begin{figure*}
    \centering
    \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{llllll}
      \toprule
      Algorithm                     & Homogeneity & Completeness & V-Measure &
      \# of clusters & Silhouette Coefficient \\
      \midrule
      K-Means                       & 0.607       & 0.592        & 0.600   & 10
      & 0.106  \\
      Affinity-Propagation          & 0.654       & 0.317        & 0.427   & 88
      & 0.082  \\
      Mean-Shift                    & 0.012       & 0.368        & 0.024   & 2
      & 0.517 \\
      Ward Agglomerative Clustering & 0.589       & 0.635        & 0.611   & 10
      & 0.097 \\
      \bottomrule
    \end{tabular}
    \end{adjustbox}
    \caption{Performance metrics of clustering algorithms.}
    \label{fig:clustering-metrics}
\end{figure*}

We first describe the experimental setup and later present information about the
dataset.
All our experiments are performed on a machine with 8-core Intel i7-3770 running
at 3.7 GHz. The system has 16 GiB DRAM. The L2 and L3 cache sizes are 256 KiB
and 8 MiB respectively.
We deploy Linux kernel 3.2.0 on the machine and disable logical processor
support.
We use the Scikit \citep{scikit-learn} framework in Python 3 for
evaluating different machine learning algorithms.
Our auto-tuning framework currently only supports Postgres 9.5. We plan to
extend it to support other database management systems like MySQL in future
work.

The dataset consists of 10 benchmark classes. These include all the benchmarks
described in \cref{tab:benchmarks}. We consider 4 variants of the YCSB key-value
store benchmark. These variants map to different read-write mixtures and are
listed below :\\

\begin{itemize}
  \item {Read-only : 100\% reads 0\% writes }
  \item {Read-heavy : 80\% reads 20\% writes }
  \item {Balanced : 50\% reads 50\% writes }
  \item {Write-heavy : 20\% reads 80\% writes } \\
\end{itemize}

We added these variants to stress-test the classifiers and estimators as these
variants can overlap with other benchmark classes along some dimensions in
high-dimensional space.

For each DBMS run, we first generate a configuration by picking
values for 12 key configuration parameters listed in \cref{tab:mutate}.
We then select a random benchmark and alter the transaction mix of the
benchmark i.e. the proportion of different types of transactions in
the specific benchmark. After running the benchmark for atleast more than
1 minute, we halt the system, and collect the dynamic features from the
DBMS and the benchmarking framework. We also collect the static features
from the specific DBMS configuration. These features and metrics constitute
one sample in the dataset.
Each sample consists of 263 features and our dataset consists of 1000
such samples.
