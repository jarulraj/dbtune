\section{Estimation} \label{sec:estimation}

We estimate expected performance of each benchmark on a given database
configuration for the second phase of our solution. In doing so, we
hope to approximately estimate the performance of the target workload
through the estimation for the benchmark it maps to. As such, we train
two estimators for each benchmark: one for database throughput and one
for database latency.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\linewidth]{figure/gp_per_benchmark_r2_scores_latency_mutate.pdf}
    \caption{Per-Benchmark Gaussian Processes to Estimate Latency}
    \label{fig:gp_r2_latency}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\linewidth]{figure/gp_per_benchmark_r2_scores_throughput_mutate.pdf}
    \caption{Per-Benchmark Gaussian Processes to Estimate Throughput}
    \label{fig:gp_r2_throughput}
\end{figure}

\begin{table*}[h!]
  \centering
  \begin{adjustbox}{max width=0.85\textwidth}
    \begin{tabular}{ll}
      \toprule
      Feature Name                   & Feature Meaning                                                     \\
      \midrule
      PG\_Cache\_Hits                & Number of buffer hits in this table                                 \\
      PG\_Index\_Hits                & Number of buffer hits in all indexes on this table                  \\
      PG\_Index\_scans               & Number of index scans initiated on this
      table                       \\
      PG\_rows\_inserted             & Number of rows inserted by queries in this database                 \\
      PG\_rows\_returned             & Number of rows returned by queries in this database                 \\
      PG\_rows\_updated              & Number of rows updated by queries in this database                  \\
      PG\_sequential\_scans          & Number of sequential scans initiated on this table                  \\
      PG\_transactions\_committed    & Number of transactions in this database that have been committed    \\
      PG\_transactions\_rolled\_back & Number of transactions in this database that have been rolled back  \\
      autovacuum                     & Number of times the autovacuum daemon vacuumed this table           \\
      commit\_delay                  & Time delay before a transaction attempts to flush the WAL buffer    \\
      fsync                          & Enable making sure that updates are physically written to disk      \\
      shared\_buffers                & Amount of memory the database server uses for shared memory buffers \\
      track\_activities              & Enable the collection of information on the executing commands      \\
      wal\_buffers                   & Amount of shared memory used for WAL data                           \\
      wal\_level                     & Determine how much information is written to the WAL                \\
      wal\_writer\_delay             & Specify the delay between activity rounds for the WAL writer        \\
      \bottomrule
    \end{tabular}
  \end{adjustbox}

  \caption{Some of the influential features for estimate throughput.}
  \label{tab:influential_features_for_throughput}
\end{table*}

\begin{table*}[h!]
  \centering
  \begin{adjustbox}{max width=0.85\textwidth}
    \begin{tabular}{ll}
      \toprule
      Feature Name          & Feature Meaning                                                    \\
      \midrule
      PG\_index\_scans      & Number of index scans initiated on this table                      \\
      PG\_sequential\_scans & Number of sequential scans initiated on this table                 \\
      fsync                 & Enable making sure that updates are physically written to disk     \\
      synchronous\_commit   & Whether transaction commit will wait for WAL records to be flushed \\
      \bottomrule
    \end{tabular}
  \end{adjustbox}

  \caption{Influential features for latency estimation.}
  \label{tab:influential_features_for_latency}
\end{table*}

We chose to use Gaussian Process Regression for this task as it works
well for estimating regressions with no prior knowledge about
distribution. It also gives a probabilistic estimation, allowing for
further insight about bounds and probability of exceeding
them. \cref{fig:gp_r2_latency} and \cref{fig:gp_r2_throughput} show
the median $\textrm{R}^2$ score across the estimators for all
benchmarks as a red line. The shaded green region indicates a single
standard deviation spread.

We infer from the figures that we can achieve a very good estimation
with as few as 600 samples. In addition, the the estimators become
collectively better as more data is provided, as evidenced by the
shrinking green region as number of samples increases.

To gain more insight about what parameters affected performance the
most, we performed Lasso Regression to determine the most influential
features. These features are summarized in
\cref{tab:influential_features_for_throughput} and
\cref{tab:influential_features_for_latency}. Confirming general
intuition, throughput seems to be greatly affected by all operation
types along with options that add delays (e.g. bgwriter\_delay,
commit\_delay) or constrain how relaxed the database can be about
transaction execution (e.g. fsync, synchronous\_commit). However, it
is interesting to note that latency seems primarily affected only by
operations that take a long time to complete and not the type of
operations or hard delays introduced into the system.

In a practical implementation of our estimator, we will not be able
to collect performance statistics before estimating performance for
different configurations. As such, we eliminated the features
pertaining to throughput and latency from consideration. However,
these features are available in the training data. We believe that a
better learner will use graphical models to infer the missing
throughput and performance information for the test data before
estimation. We will explore this technique in the future.