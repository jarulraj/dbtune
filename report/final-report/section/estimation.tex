\section{Estimation} \label{sec:estimation}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\linewidth]{figure/gp_per_benchmark_r2_scores_latency_mutate.pdf}
    \caption{Per-benchmark gaussian processes to estimate latency}
    \label{fig:gp_r2_latency}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\linewidth]{figure/gp_per_benchmark_r2_scores_throughput_mutate.pdf}
    \caption{Per-benchmark gaussian processes to estimate throughput}
    \label{fig:gp_r2_throughput}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\linewidth]{figure/wikipedia_test_hist_latency_mutate.pdf}
    \caption{Actual latency distribution for the Wikipedia benchmark}
    \label{fig:actual_latency_wikipedia}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\linewidth]{figure/wikipedia_pred_hist_latency_mutate.pdf}
    \caption{Predicted latency distribution for the Wikipedia benchmark}
    \label{fig:predicted_latency_wikipedia}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\linewidth]{figure/ycsb_balanced_test_hist_throughput_mutate.pdf}
    \caption{Actual throughput distribution for the balanced YCSB benchmark}
    \label{fig:actual_latency_ycsb_balanced}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\linewidth]{figure/ycsb_balanced_pred_hist_throughput_mutate.pdf}
    \caption{Predicted throughput distribution for the balanced YCSB benchmark}
    \label{fig:predicted_latency_ycsb_balanced}
\end{figure}

\begin{table*}[h!]
  \centering
  \begin{adjustbox}{max width=0.85\textwidth}
    \begin{tabular}{ll}
      \toprule
      Feature Name                   & Feature Meaning                                                     \\
      \midrule
      PG\_Cache\_Hits                & Number of buffer hits in this table                                 \\
      PG\_Index\_Hits                & Number of buffer hits in all indexes on this table                  \\
      PG\_Index\_scans               & Number of index scans initiated on this
      table                       \\
      PG\_rows\_inserted             & Number of rows inserted by queries in this database                 \\
      PG\_rows\_returned             & Number of rows returned by queries in this database                 \\
      PG\_rows\_updated              & Number of rows updated by queries in this database                  \\
      PG\_sequential\_scans          & Number of sequential scans initiated on this table                  \\
      PG\_transactions\_committed    & Number of transactions in this database that have been committed    \\
      PG\_transactions\_rolled\_back & Number of transactions in this database that have been rolled back  \\
      autovacuum                     & Number of times the autovacuum daemon vacuumed this table           \\
      commit\_delay                  & Time delay before a transaction attempts to flush the WAL buffer    \\
      fsync                          & Enable making sure that updates are physically written to disk      \\
      shared\_buffers                & Amount of memory the database server uses for shared memory buffers \\
      track\_activities              & Enable the collection of information on the executing commands      \\
      wal\_buffers                   & Amount of shared memory used for WAL data                           \\
      wal\_level                     & Determine how much information is written to the WAL                \\
      wal\_writer\_delay             & Specify the delay between activity rounds for the WAL writer        \\
      \bottomrule
    \end{tabular}
  \end{adjustbox}

  \caption{Some of the influential features for throughput estimation.}
  \label{tab:influential_features_for_throughput}
\end{table*}

\begin{table*}[h!]
  \centering
  \begin{adjustbox}{max width=0.85\textwidth}
    \begin{tabular}{ll}
      \toprule
      Feature Name          & Feature Meaning                                                    \\
      \midrule
      PG\_index\_scans      & Number of index scans initiated on this table                      \\
      PG\_sequential\_scans & Number of sequential scans initiated on this table                 \\
      fsync                 & Enable making sure that updates are physically written to disk     \\
      synchronous\_commit   & Whether transaction commit will wait for WAL records to be flushed \\
      \bottomrule
    \end{tabular}
  \end{adjustbox}

  \caption{Influential features for latency estimation.}
  \label{tab:influential_features_for_latency}
\end{table*}

We estimate expected performance of each benchmark on a given database
configuration for the second phase of our solution. In doing so, we
hope to approximately estimate the performance of the target workload
through the estimation for the benchmark it maps to. As such, we train
two estimators for each benchmark: one for database throughput and one
for database latency.

We chose to use Gaussian Process Regression, a supervised learning
method, for this task as it works well for estimating regressions with
no prior knowledge about distribution. It also gives a probabilistic
estimation, allowing for further insight about bounds and probability
of exceeding them. The Gaussian Process Regression models the data
with the function
\begin{equation*}
G(X) = f(X)^{\textrm{T}}\beta + Z(X)
\end{equation*}
where $f(X)^{\textrm{T}}\beta$ is a linear regression model and $Z(X)$
is a Gaussian process with zero mean. $X$ is the input, represented as
a vector of feature values.  In this model, we try to find the ``best
linear unbiased prediction'' (BLUP) of the process given the training
data. That is, we try to find the best function for
$\hat{G}(X) = G(X|\textrm{training data})$. Under the model, it can be
shown that $\hat{G}(X) = a(X)^{\textrm{T}}y$ where $a(X)$ is the
product of the weights with the feature values. From this model, we
can find the we can find the BLUP by solving the optimization problem
\begin{equation*}
a(X)^* = \arg \min\limits_{a(X)} \mathbb{E}[(G(X) - a(X)^T y)^2]
\end{equation*}
subject to the constraint $\mathbb{E}[G(X) - a(X)^T y] = 0$

One way to measure the performance of a regression model is the
$\textrm{R}^2$ score. This score is proportional to the ratio of the
likelihood of the observed data to the variance of the data. The
highest score is 1 and it indicates that the model explains the
observations very well compared to the mean of the model. Lower values
can indicate that the model does not fit the observed data as well and
that the mean is a better predictor - that is, the model does not seem
to fit the distribution that the observed values come
from. \cref{fig:gp_r2_latency} and \cref{fig:gp_r2_throughput} show
the median $\textrm{R}^2$ score across the estimators for all
benchmarks as a red line. The shaded green region indicates a single
standard deviation spread. We infer from the figures that we can
achieve a very good estimation with as few as 600 samples. In
addition, the estimators become collectively better as more data is
provided, as evidenced by the shrinking green region as number of
samples increases.

One example of a good performance estimator is the latency estimator
for the Wikipedia benchmark. This estimator has a high $\textrm{R}^2$
score of 0.9734. Looking at the actual and predicted latency values in
\cref{fig:actual_latency_wikipedia} and
\cref{fig:predicted_latency_wikipedia} respectively, we find that the
predicted distribution mimics the actual one very closely, with only a
slight dip into negative latency predictions. We believe this
estimator can be even stronger if we encode constraints for possible
predictions, such as positive values only.

One example of a poorer performance estimator is the throughput
estimator for the balanced YCSB benchmark. This estimator has a lower
$\textrm{R}^2$ score of 0.7090. Looking at the actual and predicted
throughput values in \cref{fig:actual_latency_ycsb_balanced} and
\cref{fig:predicted_latency_ycsb_balanced} respectively, we see that
the estimator fails to handle the holes in throughput seen in the
actual values. We believe these isolated throughput values are due to
environmental factors that are not reflected in the feature set such
as load on the machine and other processes' resource usages. A more
``global'' set of features across the whole system rather than just
pertaining to the database may help train better estimators.

To gain more insight about what parameters affected performance the
most, we performed Lasso Regression to determine the most influential
features. Lasso regression encourages sparse coefficients in its
solution, so it is a good method for finding the most important
features for a particular problem. It works by optimizing the
objective function
\begin{equation*}
  \min\limits_{w}
  \frac{1}{2n_{\textrm{samples}}} \|Xw - y\|_2^2 + \alpha \|w\|_1
\end{equation*}
which is a simple least squares linear regression with the weighted
lasso constraint added.

 These features are summarized in
\cref{tab:influential_features_for_throughput} and
\cref{tab:influential_features_for_latency}. Confirming general
intuition, throughput seems to be greatly affected by all operation
types along with options that add delays (e.g. bgwriter\_delay,
commit\_delay) or constrain how relaxed the database can be about
transaction execution (e.g. fsync, synchronous\_commit). However, it
is interesting to note that latency seems primarily affected only by
operations that take a long time to complete and not the type of
operations or hard delays introduced into the system.

In a practical implementation of our estimator, we will not be able
to collect performance statistics before estimating performance for
different configurations. As such, we eliminated the features
pertaining to throughput and latency from consideration. However,
these features are available in the training data. We believe that a
better learner will use graphical models to infer the missing
throughput and performance information for the test data before
estimation. We will explore this technique in the future.